{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the text database from past editions of the [Survey on Inflation and Growth Expectations](https://www.bancaditalia.it/pubblicazioni/indagine-inflazione/index.html) by Banca d'Italia in order to generate synthetic data for the fine tuning of a Large Language Model.\n",
    "\n",
    "In order to do so, we leverage the free tier of a few cloud services which provide APIs for LLM use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import dashscope\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from mistralai import Mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_excel(\"data/testi_IAI.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dicts = text_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Sei un economista che sta scrivendo una prima bozza di un documento. Ti verrà fornito un paragrafo di un testo economico nella sua versione finale e pubblicata dalla Banca d'Italia.\n",
    "\n",
    "Il tuo compito è generare una possibile versione precedente di questo testo - una bozza grezza che avrebbe potuto essere scritta prima delle revisioni e correzioni finali.\n",
    "\n",
    "La bozza che genererai dovrà contenere:\n",
    "- Ripetizioni di concetti o parole\n",
    "- Abbreviazioni tipiche di un documento di lavoro\n",
    "- Semplificazioni eccessive di concetti economici\n",
    "- Frasi più lunghe e meno strutturate\n",
    "- Qualche imprecisione terminologica\n",
    "- Un linguaggio meno formale e più colloquiale\n",
    "- Possibili ridondanze\n",
    "\n",
    "Questa bozza dovrà comunque mantenere i concetti fondamentali e i dati esatti del testo originale, ma presentandoli in forma meno raffinata e professionale.\n",
    "Non aggiungere informazioni non presenti nel testo.\n",
    "La tua risposta deve contenere solamente la possibile bozza precedente, senza alcun altro testo o informazione.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"Paragrafo nella versione finale:\n",
    "{testo}\n",
    "\n",
    "Genera una possibile bozza precedente di questo paragrafo:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alibaba Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashscope.base_http_api_url = 'https://dashscope-intl.aliyuncs.com/api/v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_results = []\n",
    "\n",
    "for item in text_dicts:\n",
    "    try:\n",
    "        time.sleep(3) # Respect API limits\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': prompt_template.format(testo=item.get(\"testo\"))}\n",
    "        ]\n",
    "        response = dashscope.Generation.call(\n",
    "            api_key=config.get('ALIBABA_API_KEY'),\n",
    "            model=\"qwen2.5-72b-instruct\", \n",
    "            messages=messages,\n",
    "            result_format='message'\n",
    "            )\n",
    "\n",
    "        qwen_results.append({**item, \"input\": response.output.choices[0].message.content})\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_df = pd.DataFrame(qwen_results)\n",
    "qwen_df.to_excel(\"data/testi_qwen_{}.xlsx\".format(datetime.now().strftime(\"%Y%m%d\")), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_model = \"mistral-large-latest\"\n",
    "\n",
    "mistral_client = Mistral(api_key=config.get('MISTRAL_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_results = []\n",
    "\n",
    "for item in text_dicts:\n",
    "    try:\n",
    "        time.sleep(3) # Respect API limits\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': prompt_template.format(testo=item.get(\"testo\"))}\n",
    "        ]\n",
    "        response = mistral_client.chat.complete(\n",
    "            model= mistral_model,\n",
    "            messages = messages\n",
    "        )\n",
    "\n",
    "        mistral_results.append({**item, \"input\": response.choices[0].message.content})\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_df = pd.DataFrame(mistral_results)\n",
    "mistral_df.to_excel(\"data/testi_mistral_{}.xlsx\".format(datetime.now().strftime(\"%Y%m%d\")), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cerebras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cerebras_client = Cerebras(\n",
    "  api_key=config.get('CEREBRAS_API_KEY'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cerebras_results = []\n",
    "\n",
    "for item in text_dicts:\n",
    "    try:\n",
    "        time.sleep(3) # Respect API limits\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': prompt_template.format(testo=item.get(\"testo\"))}\n",
    "        ]\n",
    "        response = cerebras_client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"llama-4-scout-17b-16e-instruct\",\n",
    "        )\n",
    "\n",
    "        cerebras_results.append({**item, \"input\": response.choices[0].message.content})\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cerebas_df = pd.DataFrame(cerebras_results)\n",
    "cerebas_df.to_excel(\"data/testi_cerebras_{}.xlsx\".format(datetime.now().strftime(\"%Y%m%d\")), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
